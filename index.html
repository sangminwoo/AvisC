<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AvisC</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/eyes_forest.png" alt="eyes_forest" style="width: 10%;"/>
          <h1 class="title is-1 publication-title">Don’t Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://sangminwoo.github.io/" target="_blank">Sangmin Woo</a><sup>*</sup>,
            </span>
            <span class="author-block">
              Donguk Kim<sup>*</sup>,
            </span>
            <span class="author-block">
              Jaehyuk Jang<sup>*</sup>,
            </span>
            <span class="author-block">
              Yubin Choi,
            </span>
            <span class="author-block">
              Changick Kim
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">KAIST</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.17820.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/AvisC" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.17820" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study addresses the issue observed in Large Vision Language Models (LVLMs), where excessive attention on a few image tokens, referred to as blind tokens, leads to hallucinatory responses in tasks requiring fine-grained understanding of visual objects. We found that tokens receiving lower attention weights often hold essential information for identifying nuanced object details — ranging from merely recognizing object existence to identifying their attributes (color, position, etc.) and understanding their relationships. To counteract the over-emphasis on blind tokens and to accurately respond to user queries, we introduce a technique called <strong>Attentional Vision Calibration (AVISC)</strong>. During the decoding phase, AVISC identifies blind tokens by analyzing the image-related attention distribution. It then dynamically adjusts the logits for the next token prediction by contrasting the logits conditioned on the original visual tokens with those conditioned on the blind tokens. This effectively lowers the dependency on blind tokens and promotes a more balanced consideration of all tokens. We validate AVISC on benchmarks such as POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques in mitigating object hallucinations in LVLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">Observation</h2>
        <div class="hero-body">
          <img src="static/images/observation.png" alt="Observation"/>
          <h2 class="subtitle has-text-centered">
          <strong>Attention bias in LVLMs.</strong>
          Even when the image (V) does not contain information relevant to the query (Q), LVLMs exhibit a tendency for attention to be biased towards a few image tokens (i.e., blind tokens). This phenomenon is observed by averaging the attention weights across all layers when generating the first response token.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">Motivation</h2>
        <div class="hero-body">
          <img src="static/images/motivation.png" alt="Motivation"/>
          <h2 class="subtitle has-text-centered">
          <strong>Impact of blind/non-blind tokens on prediction logits.</strong>
          <strong>(Left)</strong> Zeroing out image tokens with attention weights higher than the mean + standard deviation, i.e., blind tokens, does not significantly affect the original prediction logits, suggesting that LVLMs may assign high attention weights to tokens that do not carry significant object-discriminative information. Conversely, zeroing out non-blind tokens drastically disrupts the logits, often leading to near 50:50 probabilities, indicating a loss of object-discriminative information.
          <strong>(Right)</strong> Similarly, examples demonstrate that zeroing out non-blind tokens results in a loss of discriminative power for previously well-classified instances or produces entirely incorrect predictions, causing a significant drop in performance.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">Method: AvisC</h2>
        <div class="hero-body">
          <img src="static/images/overview.png" alt="Overview"/>
          <h2 class="subtitle has-text-centered">
          We propose a straightforward method, called AVISC, to enhance visual object understanding in LVLMs during the decoding phase. AVISC dynamically calibrates the over-emphasis on blind tokens on-the-fly at every token generation step. The calibration is guided by the attention patterns of image tokens in response to the given image and textual query. Importantly, AVISC operates without additional training, external models, or complex self-feedback mechanisms.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">POPE Results</h2>
        <div class="hero-body">
          <img src="static/images/pope.png" alt="POPE" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          AVISC consistently outperforms base decoding and other methods: VCD and M3ID. We reimplemented VCD and M3ID in our evaluation setup.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">MME-Fullset Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-fullset.png" alt="MME-Fullset" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          AVISC achieves top performance in 7 of 14 categories with InstructBLIP and in 11 categories with LLaVA-1.5. Beyond minimizing hallucinations, AVISC also boosts the general functionality of LVLMs.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">MME-Hallucination Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-hallucination.png" alt="MME-Hallucination" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          Our method effectively reduces hallucinations at both object and attribute levels, surpassing VCD and M3ID in Total Score.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-2">AMBER Results</h2>
        <div class="hero-body">
          <img src="static/images/amber.png" alt="AMBER" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          AVISC outperforms contrastive decoding baselines in both generative and discriminative tasks, achieving the highest AMBER score.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="hero-body">
          <img src="static/images/amber_discriminative.png" alt="AMBER Discriminative" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          Our demonstrates superior performance overall, particularly excelling in the Existence and Action categories in both InstructBLIP and LLaVA-1.5.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/llava_bench.png" alt="LLaVA Bench Results"/>
              <h2 class="subtitle has-text-centered">
              <strong>LLaVA-Bench.</strong>
              Hallucinations are highlighted in red.
            </div>
            <div class="item">
              <img src="static/images/qualitative_amber_instructblip.png" alt="AMBER InstructBLIP"/>
              <h2 class="subtitle has-text-centered">
              <strong>AMBER InstructBLIP.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/qualitative_amber_llava.png" alt="AMBER LLaVA-1.5"/>
              <h2 class="subtitle has-text-centered">
              <strong>AMBER LLaVA-1.5.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/qualitative_mme_instructblip.png" alt="MME InstructBLIP"/>
              <h2 class="subtitle has-text-centered">
              <strong>MME InstructBLIP.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/qualitative_mme_llava.png" alt="MME LLaVA-1.5"/>
              <h2 class="subtitle has-text-centered">
              <strong>MME LLaVA-1.5.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/qualitative_pope.png" alt="POPE"/>
              <h2 class="subtitle has-text-centered">
              <strong>POPE.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{{woo2024dont,
  title={Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models}, 
  author={Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17820},
  year={2024},
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->



<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</body>
</html>